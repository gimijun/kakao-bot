from flask import Flask, jsonify, request
import feedparser
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta, timezone
import json
import sys # sys ëª¨ë“ˆ ì„í¬íŠ¸ (print í”ŒëŸ¬ì‹œìš©)
import urllib.parse # URL ë””ì½”ë”©ì„ ìœ„í•´ ì¶”ê°€
import time # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ time ëª¨ë“ˆ ì„í¬íŠ¸

app = Flask(__name__)

# JSON íŒŒì¼ë¡œë¶€í„° ì§€ì—­ â†’ ì¢Œí‘œ ì •ë³´ ë¡œë“œ
# ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ì´ íŒŒì¼ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆê±°ë‚˜, Render ì„¤ì •ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
try:
    with open("region_coords.json", encoding="utf-8") as f:
        region_coords = json.load(f)
except FileNotFoundError:
    print("Warning: region_coords.json not found. Weather functionality may be limited.")
    sys.stdout.flush()
    region_coords = {} # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”í•˜ì—¬ NameError ë°©ì§€

def get_coords(region_name):
    """ì§€ì—­ ì´ë¦„ìœ¼ë¡œ ì¢Œí‘œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    # region_coords ë”•ì…”ë„ˆë¦¬ì— 'êµ¬'ë‚˜ 'ì‹œ'ê°€ í¬í•¨ëœ ì „ì²´ ì§€ì—­ëª…ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
    # ì •í™•í•œ ë§¤ì¹­ì„ ìœ„í•´ ì…ë ¥ëœ region_nameì„ ê¸°ë°˜ìœ¼ë¡œ ì°¾ìŒ
    # ì˜ˆë¥¼ ë“¤ì–´, 'ì„œìš¸'ì´ ì…ë ¥ë˜ë©´ 'ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬'ì™€ ê°™ì€ ìƒì„¸ ì£¼ì†Œë¥¼ ë§¤í•‘í•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” region_coords.jsonì— ì €ì¥ëœ í‚¤ë“¤ì„ ìˆœíšŒí•˜ë©° ì¼ì¹˜í•˜ëŠ” ì§€ì—­ì„ ì°¾ìŠµë‹ˆë‹¤."""
    
    # ë¨¼ì € ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì§€ì—­ì„ ì°¾ìŒ
    if region_name in region_coords:
        return region_coords[region_name]

    # ì…ë ¥ëœ ì§€ì—­ëª…ì´ í¬í•¨ëœ ë” ìƒì„¸í•œ ì§€ì—­ì„ ì°¾ìŒ (ì˜ˆ: "ì„œìš¸" -> "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬" ë“±)
    # 'region_name'ì´ 'full_region_name'ì˜ ì¼ë¶€ì¸ ê²½ìš° ë˜ëŠ” ì‹œ/ë„ ì´ë¦„ë§Œ ì…ë ¥ëœ ê²½ìš°ë¥¼ ì²˜ë¦¬
    for full_region_name, coords in region_coords.items():
        # ì˜ˆë¥¼ ë“¤ì–´, region_nameì´ "ì„œìš¸"ì¼ ë•Œ "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬"ë¥¼ ì°¾ê¸° ìœ„í•¨
        # ë˜ëŠ” region_nameì´ "ì¢…ë¡œêµ¬"ì¼ ë•Œ "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬"ë¥¼ ì°¾ê¸° ìœ„í•¨
        if region_name in full_region_name:
            print(f"Found partial match for '{region_name}': '{full_region_name}' -> {coords}")
            sys.stdout.flush()
            return coords
            
    print(f"Coords not found for region: {region_name}")
    sys.stdout.flush()
    return None, None


# SERVICE_KEYëŠ” fetch_weather_data í•¨ìˆ˜ ë‚´ë¶€ì— ì§ì ‘ í•˜ë“œì½”ë”© (ì‚¬ìš©ì ìš”ì²­)

def extract_image_from_entry(entry):
    """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if hasattr(entry, 'media_content'):
        for media in entry.media_content:
            if 'url' in media:
                return media['url']
    return "https://t1.daumcdn.net/media/img-section/news_card_default.png"

def fetch_rss_news(rss_url, max_count=5):
    """ì§€ì •ëœ RSS URLì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    start_time = time.time() # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    try:
        feed = feedparser.parse(rss_url)
        news_items = []
        for entry in feed.entries[:max_count]:
            # HTML íƒœê·¸ ì œê±° ë° ì œëª© ì •ë¦¬
            title = re.sub(r'<[^>]+>', '', entry.title)
            image = extract_image_from_entry(entry)
            link = entry.link
            news_items.append({
                "title": title,
                "image": image,
                "link": link
            })
        end_time = time.time() # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        print(f"fetch_rss_news from {rss_url} took {end_time - start_time:.2f} seconds.")
        sys.stdout.flush()
        return news_items
    except Exception as e:
        print(f"Error fetching RSS news from {rss_url}: {e}")
        sys.stdout.flush()
        return []

def clean_image_url(image):
    """ìƒëŒ€ ê²½ë¡œ ì´ë¯¸ì§€ URLì„ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if image.startswith("//"):
        return "https:" + image
    elif image.startswith("/"):
        # ë™ì•„ì¼ë³´ íŠ¹ì • ë„ë©”ì¸ì— ëŒ€í•œ ì²˜ë¦¬
        return "https://www.donga.com" + image
    return image

def fetch_donga_search_news(keyword, max_count=5):
    """ë™ì•„ì¼ë³´ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    start_time = time.time() # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    # url = f"https://www.donga.com/news/search?query={keyword}"
    # headers = {
    #     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    #     "Accept-Language": "ko-KR,ko;q=0.9",
    #     "Referer": "https://www.donga.com/"
    # }
    # try:
    #     res = requests.get(url, headers=headers, timeout=5) # Timeout 5ì´ˆë¡œ ë³€ê²½
    #     res.raise_for_status() # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
    #     soup = BeautifulSoup(res.text, "html.parser")
        
    #     news_items = []
        
    #     # ê²€ìƒ‰ í˜ì´ì§€ì˜ ê¸°ì‚¬ ëª©ë¡ ì…€ë ‰í„° ê°•í™”
    #     # 'ul.row_list li article'ì´ ê°€ì¥ í”í•œ íŒ¨í„´ì´ì§€ë§Œ, ë‹¤ë¥¸ ê°€ëŠ¥ì„±ë„ ê³ ë ¤
    #     potential_articles = soup.select("ul.row_list li article")
    #     if not potential_articles:
    #         potential_articles = soup.select("ul.row_list li") # article íƒœê·¸ê°€ ì—†ì„ ê²½ìš° lië§Œ ì„ íƒ

    #     for item in potential_articles[:max_count]:
    #         title_tag = item.select_one("h4")
    #         link_tag = item.select_one("a")
    #         image_tag = item.select_one("img") # ì´ë¯¸ì§€ íƒœê·¸ë¥¼ ì¢€ ë” ë„“ê²Œ ì°¾ìŒ
    #         if not image_tag: # í˜¹ì‹œ div ë‚´ë¶€ì— ìˆì„ ê²½ìš°
    #             image_tag = item.select_one("div.thumb img") 
    #         if not image_tag: # ë˜ ë‹¤ë¥¸ í”í•œ íŒ¨í„´
    #             image_tag = item.select_one("header a div img")


    #         title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    #         link = link_tag["href"] if link_tag and link_tag.has_attr("href") else "#"
            
    #         # ë§í¬ê°€ ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    #         if link.startswith('//'): 
    #             link = "https:" + link
    #         elif link.startswith('/'):
    #              link = "https://www.donga.com" + link

    #         image = ""
    #         if image_tag:
    #             image = image_tag.get("src") or image_tag.get("data-src") or ""
    #             image = clean_image_url(image)
    #         else:
    #             image = "https://via.placeholder.com/200" # ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©

    #         # ìœ íš¨í•œ ì œëª©ê³¼ ë§í¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
    #         if title != "ì œëª© ì—†ìŒ" and link != "#": 
    #             news_items.append({
    #                 "title": title,
    #                 "image": image,
    #                 "link": link
    #             })
        
    #     if not news_items and len(potential_articles) > 0:
    #         print(f"Warning: Could not extract valid news items from search page for '{keyword}'. Potentially broken selectors for title/link within found articles/list items. Found {len(potential_articles)} potential items.")
    #         sys.stdout.flush()

    """RSS í”¼ë“œ ê¸°ë°˜ ë‰´ìŠ¤ ListCard ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    articles = fetch_rss_news('https://rss.donga.com/total.xml', 20)
    if not articles:
        items = [{
            "title": f"{title} ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "imageUrl": "https://via.placeholder.com/200",
            "link": {"web": web_url}
        }]
    else:
        items = [{
            "title": a["title"],
            "imageUrl": a["image"],
            "link": {"web": a["link"]}
        } for a in articles]

    for item in items[:10]:
        from google import genai

        import base64

        code = 'QUl6YVN5QXdCX0UzZVdPbk1tbzBNWmNHYUlGaXBiM0plcVozMEM4'
        code_bytes = code.encode('ascii')
        
        decoded = base64.b64decode(code_bytes)
        str = decoded.decode('UTF-8')
        
        client = genai.Client(api_key=str)
        
        try:
            response = client.models.generate_content(
            model="gemini-2.5-flash-preview-05-20", contents=f"{item['title']}ê³¼ {keyword} ì‚¬ì´ì˜ ì—°ê´€ì„±ì„ í¼ì„¼í…Œì´ì§€(%)ë¡œ ë‚˜íƒ€ë‚´ì„¸ìš”. í¼ì„¼í…Œì´ì§€ ì™¸ì—ëŠ” ì ˆëŒ€ ì•„ë¬´ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."
        )
            print(response.text, keyword)
        except:
            print('failed. stop doing.')
            break
        
    return jsonify({
        "version": "2.0",
        "useCallback" : True,
        "data": {
            "text" : "ê²€ìƒ‰ ì¤‘ì´ì—ìš”ğŸ˜˜"
        },
        "template": {
            "outputs": [{
                "listCard": {
                    "header": 'í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼',
                    "items": response.text,
                    "buttons": [{
                        "label": "ë”ë³´ê¸°",
                        "action": "webLink",
                        "webLinkUrl": web_url
                    }]
                }
            }],
            "quickReplies": common_quick_replies(topic=title) 
        }
    })

    end_time = time.time() # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    print(f"fetch_donga_search_news for '{keyword}' took {end_time - start_time:.2f} seconds.")
    sys.stdout.flush()
    return news_items
    # except requests.exceptions.RequestException as e:
    #     print(f"Error fetching Donga search news for '{keyword}': {e}")
    #     sys.stdout.flush()
    #     return []
    # except Exception as e:
    #     print(f"Error parsing Donga search news for '{keyword}': {e}")
    #     sys.stdout.flush()
    #     return []

def fetch_donga_trending_news(url, max_count=5):
    """ë™ì•„ì¼ë³´ì—ì„œ íŠ¸ë Œë”© ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    start_time = time.time() # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        res = requests.get(url, headers=headers, timeout=5) # Timeout 5ì´ˆë¡œ ë³€ê²½
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        
        news_items = []
        
        # ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì…€ë ‰í„°ë¥¼ ì‹œë„
        # "ë§ì´ ë³¸ ë‰´ìŠ¤"ë‚˜ "ìš”ì¦˜ ëœ¨ëŠ” ì´ìŠˆ" í˜ì´ì§€ëŠ” article íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ
        potential_articles = []
        
        # ê°€ì¥ í”í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        selectors_to_try = [
            "ul.row_list li article", # ê¸°ì¡´ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ì‚¬ìš©í•˜ë˜ íŒ¨í„´
            "div.list ul li article",  # ê¸°ì¡´ íŠ¸ë Œë”© í˜ì´ì§€ì—ì„œ ì‚¬ìš©í•˜ë˜ íŒ¨í„´
            "ul.article_list_type01 li",
            "div.list_type01 ul li",
            "ul.type_list li",
            "div.news_list li",
            "section.ranking_type01 li", # ë­í‚¹ ì„¹ì…˜ íŒ¨í„´
            "ul li" # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ê°€ì¥ ë„“ì€ ë²”ìœ„
        ]
        
        for selector in selectors_to_try:
            found_items = soup.select(selector)
            if found_items:
                potential_articles = found_items
                print(f"Found articles with selector: {selector}")
                sys.stdout.flush()
                break # ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•ŠìŒ
        
        if not potential_articles:
            print(f"Warning: No potential articles found using any selector for URL: {url}")
            sys.stdout.flush()


        for item in potential_articles[:max_count]:
            title_tag = item.select_one("h4 a") # h4 íƒœê·¸ ì•ˆì— a íƒœê·¸ê°€ ìˆì„ ê°€ëŠ¥ì„±
            if not title_tag:
                title_tag = item.select_one("a.link_news") # ë‰´ìŠ¤ ë§í¬ í´ë˜ìŠ¤
            if not title_tag:
                title_tag = item.select_one("a") # ê°€ì¥ ì¼ë°˜ì ì¸ a íƒœê·¸

            link_tag = item.select_one("a") # ë§í¬ëŠ” ë³´í†µ a íƒœê·¸ ìì²´

            image_tag = item.select_one("img") 
            if not image_tag: # íŠ¹ì • í´ë˜ìŠ¤ê°€ ìˆëŠ” ì´ë¯¸ì§€ íƒœê·¸ë¥¼ ì‹œë„
                image_tag = item.select_one("img.news_thumb") 
            if not image_tag:
                image_tag = item.select_one("div.thumb img") # ì¸ë„¤ì¼ ì´ë¯¸ì§€ê°€ div.thumb ì•ˆì— ìˆì„ ê²½ìš°
            if not image_tag:
                image_tag = item.select_one("header a img") # ê¸°ì¡´ íŒ¨í„´

            title = ""
            if title_tag and not isinstance(title_tag, str): # title_tagê°€ Tag ê°ì²´ì¸ì§€ í™•ì¸
                title = title_tag.get_text(strip=True)
            else:
                title = "ì œëª© ì—†ìŒ"

            link = ""
            if link_tag and hasattr(link_tag, 'get') and link_tag.has_attr("href"): # link_tagê°€ Tag ê°ì²´ì¸ì§€ í™•ì¸
                link = link_tag["href"]
            else:
                link = "#"
            
            # ë§í¬ê°€ ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if link.startswith('//'): 
                link = "https:" + link
            elif link.startswith('/'):
                 link = "https://www.donga.com" + link

            image = ""
            if image_tag and hasattr(image_tag, 'get'): # image_tagê°€ Tag ê°ì²´ì¸ì§€ í™•ì¸
                image = image_tag.get("src") or image_tag.get("data-src") or ""
                image = clean_image_url(image)
            else:
                image = "https://via.placeholder.com/200" # ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©

            # ìœ íš¨í•œ ì œëª©ê³¼ ë§í¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            if title != "ì œëª© ì—†ìŒ" and link != "#": 
                news_items.append({
                    "title": title,
                    "image": image,
                    "link": link
                })
        
        if not news_items and len(potential_articles) > 0:
            print(f"Warning: Could not extract valid news items from trending page {url}. Potentially broken selectors for title/link within found articles/list items. Found {len(potential_articles)} potential items, but no valid news_items were created.")
            sys.stdout.flush()

        end_time = time.time() # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        print(f"fetch_donga_trending_news from {url} took {end_time - start_time:.2f} seconds.")
        sys.stdout.flush()
        return news_items
    except requests.exceptions.RequestException as e:
        print(f"Error fetching Donga trending news from {url}: {e}")
        sys.stdout.flush()
        return []
    except Exception as e:
        print(f"Error parsing Donga trending news from {url}: {e}. Raw HTML snippet (first 500 chars): {res.text[:500] if res else 'No response'}")
        sys.stdout.flush()
        return []


def common_quick_replies(topic=None): 
    """ëª¨ë“  ë‰´ìŠ¤ ì‘ë‹µì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë  Quick Repliesë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    quick_replies_list = [
        {
            "label": "ì•Œë¦¼ë°›ê¸°",
            "action": "block",
            "blockId": "6848b46a938bdf47fcf3b4dc", 
            "context": { # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                "name": "news_alarm_context", # ì»¨í…ìŠ¤íŠ¸ ì´ë¦„
                "lifeSpan": 3, # 3í„´ ë™ì•ˆ ìœ ì§€
                "params": {
                    "topic": topic # ì»¨í…ìŠ¤íŠ¸ì— topic ì €ì¥
                }
            }
        },
        {"label": "ê²€ìƒ‰", "action": "message", "messageText": "ê²€ìƒ‰", "blockId": "6840fd4cc5b310190b70166a"},
        {"label": "ì •ì¹˜", "action": "message", "messageText": "ì •ì¹˜", "blockId": "683596834df7f67fcdd66b62"},
        {"label": "ê²½ì œ", "action": "message", "messageText": "ê²½ì œ", "blockId": "683596b798b6403c8dad6138"},
        {"label": "ì‚¬íšŒ", "action": "message", "messageText": "ì‚¬íšŒ", "blockId": "683596c0e7598b00aa7e6eec"},
        {"label": "ë¬¸í™”", "action": "message", "messageText": "ë¬¸í™”", "blockId": "683596e8d9c3e21ccc39943b"},
        {"label": "êµ­ì œ", "action": "message", "messageText": "êµ­ì œ", "blockId": "683597142c50e1482b1e05db"},
        {"label": "IT ê³¼í•™", "action": "message", "messageText": "IT ê³¼í•™", "blockId": "68359701d9c3e21ccc399440"},
        {"label": "ìŠ¤í¬ì¸ ", "action": "message", "messageText": "ìŠ¤í¬ì¸ ", "blockId": "68359725938bdf47fcf0d8a4"},
        {"label": "ì—°ì˜ˆ", "action": "message", "messageText": "ì—°ì˜ˆ", "blockId": "683597362c50e1482b1e05df"} 
    ]
    return quick_replies_list


def list_card_response(title, rss_url, web_url):
    """RSS í”¼ë“œ ê¸°ë°˜ ë‰´ìŠ¤ ListCard ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    articles = fetch_rss_news(rss_url)
    if not articles:
        items = [{
            "title": f"{title} ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "imageUrl": "https://via.placeholder.com/200",
            "link": {"web": web_url}
        }]
    else:
        items = [{
            "title": a["title"],
            "imageUrl": a["image"],
            "link": {"web": a["link"]}
        } for a in articles]

    return jsonify({
        "version": "2.0",
        "template": {
            "outputs": [{
                "listCard": {
                    "header": {"title": f"{title} ë‰´ìŠ¤ TOP {len(items)}"},
                    "items": items,
                    "buttons": [{
                        "label": "ë”ë³´ê¸°",
                        "action": "webLink",
                        "webLinkUrl": web_url
                    }]
                }
            }],
            "quickReplies": common_quick_replies(topic=title) 
        }
    })

def trending_card_response(title, web_url):
    """íŠ¸ë Œë”© ë‰´ìŠ¤ ListCard ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    articles = fetch_donga_trending_news(web_url)
    if not articles:
        items = [{
            "title": f"{title} ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "imageUrl": "https://via.placeholder.com/200",
            "link": {"web": web_url}
        }]
    else:
        items = [{
            "title": a["title"],
            "imageUrl": a["image"],
            "link": {"web": a["link"]}
        } for a in articles]

    return jsonify({
        "version": "2.0", 
        "template": {
            "outputs": [{
                "listCard": {
                    "header": {"title": f"{title} TOP {len(items)}"},
                    "items": items,
                    "buttons": [{
                        "label": "ë”ë³´ê¸°",
                        "action": "webLink",
                        "webLinkUrl": web_url
                    }]
                }
            }]
        },
        "quickReplies": common_quick_replies(topic=title) 
    })

def search_news_response(keyword, max_count=5):
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ë‰´ìŠ¤ ListCard ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    articles = fetch_donga_search_news(keyword, max_count=max_count)
    if not articles:
        items = [{
            "title": f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "imageUrl": "https://via.placeholder.com/200",
            "link": {"web": f"https://www.donga.com/news/search?query={keyword}"}
        }]
    else:
        items = [{
            "title": a["title"],
            "imageUrl": a["image"],
            "link": {"web": a["link"]}
        } for a in articles]

    return jsonify({
        "version": "2.0",
        "template": {
            "outputs": [{
                "listCard": {
                    "header": {"title": f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼"},
                    "items": items,
                    "buttons": [{
                        "label": "ë”ë³´ê¸°",
                        "action": "webLink",
                        "webLinkUrl": f"https://www.donga.com/news/search?query={keyword}"
                    }]
                }
            }],
            "quickReplies": common_quick_replies(topic=keyword) 
        }
    })

# --- ë‚ ì”¨ ê´€ë ¨ í•¨ìˆ˜ ë° ë¼ìš°íŠ¸ ---

def get_fine_dust_level(pm_value, is_pm25=False):
    """ë¯¸ì„¸ë¨¼ì§€/ì´ˆë¯¸ì„¸ë¨¼ì§€ ë†ë„ì— ë”°ë¥¸ 5ë‹¨ê³„ ë“±ê¸‰ê³¼ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        pm = float(pm_value)
        if is_pm25: # ì´ˆë¯¸ì„¸ë¨¼ì§€ (PM2.5) ê¸°ì¤€
            if pm <= 8:
                return "ë§¤ìš°ì¢‹ìŒ", "ë§¤ìš° ì²­ì •í•˜ê³  ìƒì¾Œí•´ìš”!"
            elif pm <= 15:
                return "ì¢‹ìŒ", "ë§‘ì€ ê³µê¸° ë§ˆì‹œë©° í™œë™í•˜ê¸° ì¢‹ì•„ìš”."
            elif pm <= 35:
                return "ë³´í†µ", "ë³´í†µ ìˆ˜ì¤€ì˜ ê³µê¸° ì§ˆì…ë‹ˆë‹¤."
            elif pm <= 75:
                return "ë‚˜ì¨", "ì‹¤ì™¸ í™œë™ ì‹œ ë§ˆìŠ¤í¬ ì°©ìš©ì„ ê¶Œì¥í•´ìš”."
            else:
                return "ë§¤ìš°ë‚˜ì¨", "ëª¨ë“  ì—°ë ¹ëŒ€ ì‹¤ì™¸ í™œë™ ìì œ!"
        else: # ë¯¸ì„¸ë¨¼ì§€ (PM10) ê¸°ì¤€
            if pm <= 15:
                return "ë§¤ìš°ì¢‹ìŒ", "ë§¤ìš° ì²­ì •í•˜ê³  ìƒì¾Œí•´ìš”!"
            elif pm <= 30:
                return "ì¢‹ìŒ", "ì•¼ì™¸ í™œë™í•˜ê¸° ì¢‹ì•„ìš”."
            elif pm <= 80:
                return "ë³´í†µ", "ë³´í†µ ìˆ˜ì¤€ì˜ ê³µê¸° ì§ˆì…ë‹ˆë‹¤."
            elif pm <= 150:
                return "ë‚˜ì¨", "ë§ˆìŠ¤í¬ ì°©ìš©ì„ ê¶Œì¥í•´ìš”."
            else:
                return "ë§¤ìš°ë‚˜ì¨", "ëª¨ë“  ì—°ë ¹ëŒ€ ì•¼ì™¸ í™œë™ ìì œ!"
    except (ValueError, TypeError):
        return "ì •ë³´ ì—†ìŒ", "ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def get_humidity_level(reh_value):
    """ìŠµë„ì— ë”°ë¥¸ 5ë‹¨ê³„ ë“±ê¸‰ê³¼ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        reh = float(reh_value)
        if reh <= 30:
            return "ë§¤ìš°ë‚®ìŒ", "ê±´ì¡°í•œ ë‚ ì”¨! í”¼ë¶€ ë³´ìŠµì— ì‹ ê²½ ì¨ì£¼ì„¸ìš”."
        elif reh <= 40:
            return "ë‚®ìŒ", "í”¼ë¶€ê°€ ê±´ì¡°í•´ì§ˆ ìˆ˜ ìˆì–´ìš”."
        elif reh <= 60:
            return "ë³´í†µ", "ì¾Œì í•œ ìŠµë„ì…ë‹ˆë‹¤."
        elif reh <= 75:
            return "ë†’ìŒ", "ìŠµí•œ ë‚ ì”¨ê°€ ì˜ˆìƒë©ë‹ˆë‹¤."
        else:
            return "ë§¤ìš°ë†’ìŒ", "ë¶ˆì¾Œì§€ìˆ˜ê°€ ë†’ì„ ìˆ˜ ìˆì–´ìš”. ì œìŠµì— ì‹ ê²½ ì“°ì„¸ìš”!"
    except (ValueError, TypeError):
        return "ì •ë³´ ì—†ìŒ", "ìŠµë„ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def get_sky_condition(sky_code, pty_code):
    """í•˜ëŠ˜ ìƒíƒœ(SKY)ì™€ ê°•ìˆ˜ í˜•íƒœ(PTY) ì½”ë“œë¥¼ í•œê¸€ ì„¤ëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    sky_dict = {
        "1": "ë§‘ìŒ",
        "3": "êµ¬ë¦„ë§ìŒ",
        "4": "íë¦¼"
    }
    pty_dict = {
        "0": "", # ê°•ìˆ˜ ì—†ìŒ
        "1": "ë¹„",
        "2": "ë¹„/ëˆˆ",
        "3": "ëˆˆ",
        "4": "ì†Œë‚˜ê¸°",
        "5": "ë¹—ë°©ìš¸",
        "6": "ë¹—ë°©ìš¸/ëˆˆë‚ ë¦¼",
        "7": "ëˆˆë‚ ë¦¼"
    }
    
    sky_desc = sky_dict.get(str(sky_code), "ì•Œ ìˆ˜ ì—†ìŒ")
    pty_desc = pty_dict.get(str(pty_code), "")

    if pty_desc:
        return pty_desc # ê°•ìˆ˜ í˜•íƒœê°€ ìˆìœ¼ë©´ ê°•ìˆ˜ í˜•íƒœ ìš°ì„ 
    return sky_desc # ê°•ìˆ˜ í˜•íƒœê°€ ì—†ìœ¼ë©´ í•˜ëŠ˜ ìƒíƒœ

def get_latest_base_time(current_time):
    """
    ê¸°ìƒì²­ ì´ˆë‹¨ê¸°ì‹¤í™© APIì˜ base_timeì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    APIëŠ” 10ë¶„ ë‹¨ìœ„ë¡œ ìë£Œê°€ ìƒì‚°ë˜ë©°, ì •ì‹œ ê¸°ì¤€ 40ë¶„ í›„ ë°œí‘œë©ë‹ˆë‹¤.
    (ì˜ˆ: 09ì‹œ 20ë¶„ ìë£ŒëŠ” 10ì‹œ 00ë¶„ì— ë°œí‘œ)
    """
    # 40ë¶„ ì „ ì‹œê°„ ê³„ì‚° (í˜„ì¬ ì‹œê°ìœ¼ë¡œë¶€í„° 40ë¶„ì„ ëº€ ì‹œê°ì´ ì‹¤ì œ ê´€ì¸¡ ì‹œê°ì´ ë¨)
    adjusted_time = current_time - timedelta(minutes=40)
    
    # ë¶„ì„ 10ë¶„ ë‹¨ìœ„ë¡œ ë‚´ë¦¼ (ì˜ˆ: 05:52 -> 05:50)
    base_minute = (adjusted_time.minute // 10) * 10
    
    # ì´ˆì™€ ë§ˆì´í¬ë¡œì´ˆëŠ” 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì •í™•í•œ base_time (HHMM)ì„ ë§Œë“­ë‹ˆë‹¤.
    base_datetime = adjusted_time.replace(minute=base_minute, second=0, microsecond=0)
    
    return base_datetime.strftime("%Y%m%d"), base_datetime.strftime("%H%M")


def fetch_weather_data(nx, ny, region_full_name="ì„œìš¸"):
    """
    ê¸°ìƒì²­ APIì—ì„œ ë‚ ì”¨ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³ , ì—ì–´ì½”ë¦¬ì•„ APIì—ì„œ ë¯¸ì„¸ë¨¼ì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    start_time = time.time() # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    # ê¸°ìƒì²­ API ì„œë¹„ìŠ¤ í‚¤ (ë””ì½”ë”©ëœ í‚¤ ì‚¬ìš©)
    # ì´ ë¶€ë¶„ì„ ë°œê¸‰ë°›ìœ¼ì‹  API í‚¤ë¡œ êµì²´í•´ì£¼ì„¸ìš”!
    weather_service_key_encoded = "N%2FRBXLEXYr%2FO1xxA7qcJZY5LK63c1D44dWsoUszF%2BDHGpY%2Bn2xAea7ruByvKh566Qf69vLarJBgGRXdVe4DlkA%3D%3D"
    weather_service_key = urllib.parse.unquote(weather_service_key_encoded) # ëª…ì‹œì  ë””ì½”ë”©
    
    # ì—ì–´ì½”ë¦¬ì•„ API ì„œë¹„ìŠ¤ í‚¤ (ë””ì½”ë”©ëœ í‚¤ ì‚¬ìš©)
    # ì´ ë¶€ë¶„ì„ ë°œê¸‰ë°›ìœ¼ì‹  API í‚¤ë¡œ êµì²´í•´ì£¼ì„¸ìš”!
    airkorea_service_key_encoded = "N%2FRBXLEXYr%2FO1xxA7qcJZY5LK63c1D44dWsoUszF%2BDHGpY%2Bn2xAea7ruByvKh566Qf69vLarJBgGRXdVe4DlkA%3D%3D"
    airkorea_service_key = urllib.parse.unquote(airkorea_service_key_encoded) # ëª…ì‹œì  ë””ì½”ë”©

    weather = {}

    print(f"--- Starting fetch_weather_data for region: {region_full_name} ---")
    sys.stdout.flush()

    # requests sessionì„ ì‚¬ìš©í•˜ì—¬ SSL ë¬¸ì œ íšŒí”¼ ì‹œë„
    session = requests.Session()
    # ì—ì–´ì½”ë¦¬ì•„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” ìœ ì§€
    session.verify = False 

    try:
        # 1. ê¸°ìƒì²­ ì´ˆë‹¨ê¸° ì‹¤í™© API í˜¸ì¶œ
        # Render ì„œë²„ê°€ UTCë¡œ ì„¤ì •ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ, KSTë¡œ ë³€í™˜
        KST = timezone(timedelta(hours=9))
        now_kst = datetime.now(KST)

        base_date, base_time = get_latest_base_time(now_kst.replace(tzinfo=None)) # get_latest_base_timeì— naive datetime ì „ë‹¬

        weather_url = "http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getUltraSrtNcst"
        weather_params = {
            "serviceKey": weather_service_key, 
            "pageNo": "1",
            "numOfRows": "100",
            "dataType": "JSON",
            "base_date": base_date,
            "base_time": base_time,
            "nx": nx,
            "ny": ny
        }

        print(f"Calling KMA API with base_date={base_date}, base_time={base_time}, nx={nx}, ny={ny}")
        sys.stdout.flush()
        kma_api_start_time = time.time()
        weather_res = session.get(weather_url, params=weather_params, timeout=5) # Timeout 5ì´ˆë¡œ ë³€ê²½
        kma_api_end_time = time.time()
        print(f"KMA API call took {kma_api_end_time - kma_api_start_time:.2f} seconds. Status Code: {weather_res.status_code}")
        sys.stdout.flush()
        weather_res.raise_for_status() # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        weather_data_json = weather_res.json()

        if weather_data_json.get('response', {}).get('header', {}).get('resultCode') == '00':
            weather_items = weather_data_json['response']['body']['items']['item']
            for item in weather_items:
                category = item['category']
                value = item['obsrValue']
                if category in ["T1H", "REH", "SKY", "PTY"]: 
                    weather[category] = value
            print(f"Successfully fetched KMA weather data: {weather}")
            sys.stdout.flush()
        else:
            error_msg = weather_data_json.get('response', {}).get('header', {}).get('resultMsg', 'ì•Œ ìˆ˜ ì—†ëŠ” ê¸°ìƒì²­ ì˜¤ë¥˜')
            print(f"KMA API error: {error_msg}. Full Response: {json.dumps(weather_data_json, indent=2)}")
            sys.stdout.flush()

    except requests.exceptions.RequestException as e:
        print(f"Error fetching weather data from KMA API: {e}")
        sys.stdout.flush()
    except Exception as e:
        print(f"Error processing KMA weather data: {e}")
        sys.stdout.flush()

    try:
        # 2. ì—ì–´ì½”ë¦¬ì•„ ëŒ€ê¸°ì˜¤ì—¼ì •ë³´ ì¡°íšŒ API í˜¸ì¶œ (ì‹œë„ë³„ ì‹¤ì‹œê°„ ì¸¡ì •ì •ë³´)
        # sidoNameì„ ìœ„í•œ ë§¤í•‘: region_full_nameì—ì„œ ê´‘ì—­ ì‹œë„ëª… ì¶”ì¶œ
        main_sido_part = region_full_name.split(' ')[0]
        sido_mapping = {
            "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸", "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬",
            "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ", "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼", "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „",
            "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ì„¸ì¢…", "ê²½ê¸°ë„": "ê²½ê¸°",
            "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "ê°•ì›", "ì¶©ì²­ë¶ë„": "ì¶©ë¶", "ì¶©ì²­ë‚¨ë„": "ì¶©ë‚¨",
            "ì „ë¼ë¶ë„": "ì „ë¶", "ì „ë¼ë‚¨ë„": "ì „ë‚¨", "ê²½ìƒë¶ë„": "ê²½ë¶",
            "ê²½ìƒë‚¨ë„": "ê²½ë‚¨", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "ì œì£¼"
        }
        # ë§¤í•‘ëœ ì‹œë„ëª… ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ì—ì„œ ì¶”ì¶œí•œ ê´‘ì—­ ì‹œë„ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš© (í˜¹ì‹œëª¨ë¥¼ ì˜ˆì™¸ì²˜ë¦¬)
        airkorea_sido_name = sido_mapping.get(main_sido_part, main_sido_part)
        
        # region_coords.jsonì— ìˆëŠ” "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬" ê°™ì€ ìƒì„¸ ì´ë¦„ì´ ë“¤ì–´ì˜¬ ê²½ìš°
        # airkorea_sido_nameì— "ì„œìš¸"ë§Œ ë“¤ì–´ê°€ë„ë¡ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
        # ì´ ë¶€ë¶„ì€ sido_mappingìœ¼ë¡œ ì¶©ë¶„í•  ìˆ˜ ìˆì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„
        if "íŠ¹ë³„ì‹œ" in airkorea_sido_name or "ê´‘ì—­ì‹œ" in airkorea_sido_name or "íŠ¹ë³„ìì¹˜ì‹œ" in airkorea_sido_name or "ë„" in airkorea_sido_name:
            if "ì„œìš¸" in airkorea_sido_name: airkorea_sido_name = "ì„œìš¸"
            elif "ë¶€ì‚°" in airkorea_sido_name: airkorea_sido_name = "ë¶€ì‚°"
            elif "ëŒ€êµ¬" in airkorea_sido_name: airkorea_sido_name = "ëŒ€êµ¬"
            elif "ì¸ì²œ" in airkorea_sido_name: airkorea_sido_name = "ì¸ì²œ"
            elif "ê´‘ì£¼" in airkorea_sido_name: airkorea_sido_name = "ê´‘ì£¼"
            elif "ëŒ€ì „" in airkorea_sido_name: airkorea_sido_name = "ëŒ€ì „"
            elif "ìš¸ì‚°" in airkorea_sido_name: airkorea_sido_name = "ìš¸ì‚°"
            elif "ì„¸ì¢…" in airkorea_sido_name: airkorea_sido_name = "ì„¸ì¢…"
            elif "ê²½ê¸°" in airkorea_sido_name: airkorea_sido_name = "ê²½ê¸°"
            elif "ê°•ì›" in airkorea_sido_name: airkorea_sido_name = "ê°•ì›"
            elif "ì¶©ë¶" in airkorea_sido_name: airkorea_sido_name = "ì¶©ë¶"
            elif "ì¶©ë‚¨" in airkorea_sido_name: airkorea_sido_name = "ì¶©ë‚¨"
            elif "ì „ë¶" in airkorea_sido_name: airkorea_sido_name = "ì „ë¶"
            elif "ì „ë‚¨" in airkorea_sido_name: airkorea_sido_name = "ì „ë‚¨"
            elif "ê²½ë¶" in airkorea_sido_name: airkorea_sido_name = "ê²½ë¶"
            elif "ê²½ë‚¨" in airkorea_sido_name: airkorea_sido_name = "ê²½ë‚¨"
            elif "ì œì£¼" in airkorea_sido_name: airkorea_sido_name = "ì œì£¼"

        # ì—ì–´ì½”ë¦¬ì•„ API URLì„ HTTPë¡œ ë³€ê²½ (SSL í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° ì‹œë„)
        airkorea_url = "http://apis.data.go.kr/B552584/ArpltnInforInqireSvc/getCtprvnRltmMesureDnsty"
        airkorea_params = {
            "serviceKey": airkorea_service_key, # ë””ì½”ë”©ëœ í‚¤ ì‚¬ìš©
            "returnType": "json",
            "numOfRows": "1", 
            "pageNo": "1",
            "sidoName": airkorea_sido_name, # ì •í™•íˆ ë§¤í•‘ëœ ì‹œë„ëª… ì‚¬ìš©
            "ver": "1.3" 
        }
        
        print(f"Calling Airkorea API with sidoName={airkorea_sido_name}")
        sys.stdout.flush()
        airkorea_api_start_time = time.time()
        airkorea_res = session.get(airkorea_url, params=airkorea_params, timeout=5) # Timeout 5ì´ˆë¡œ ë³€ê²½
        airkorea_api_end_time = time.time()
        print(f"Airkorea API call took {airkorea_api_end_time - airkorea_api_start_time:.2f} seconds. Status Code: {airkorea_res.status_code}")
        sys.stdout.flush()
        airkorea_res.raise_for_status() # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        airkorea_data_json = airkorea_res.json()

        if airkorea_data_json.get('response', {}).get('header', {}).get('resultCode') == '00':
            airkorea_items = airkorea_data_json['response']['body']['items']
            if airkorea_items:
                # ì—ì–´ì½”ë¦¬ì•„ APIëŠ” ì‹œë„ ë‚´ ì—¬ëŸ¬ ì¸¡ì •ì†Œë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì²« ë²ˆì§¸ ì¸¡ì •ì†Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                # ë” ì •í™•í•˜ê²Œ í•˜ë ¤ë©´, í•´ë‹¹ ì‹œë„ ë‚´ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì¸¡ì •ì†Œë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
                first_station_data = airkorea_items[0] 
                weather['PM10'] = first_station_data.get('pm10Value')
                weather['PM25'] = first_station_data.get('pm25Value')
                print(f"Successfully fetched Airkorea data: PM10={weather.get('PM10')}, PM25={weather.get('PM25')}")
                sys.stdout.flush()
            else:
                print(f"No air quality data found for sidoName: {airkorea_sido_name}. Check API response structure or data availability for this region.")
                sys.stdout.flush()
        else:
            error_msg = airkorea_data_json.get('response', {}).get('header', {}).get('resultMsg', 'ì•Œ ìˆ˜ ì—†ëŠ” ì—ì–´ì½”ë¦¬ì•„ ì˜¤ë¥˜')
            print(f"Airkorea API error: {error_msg}. Full Response: {json.dumps(airkorea_data_json, indent=2)}")
            sys.stdout.flush()

    except requests.exceptions.RequestException as e:
        print(f"Error fetching airkorea data: {e}")
        sys.stdout.flush()
    except Exception as e:
        print(f"Error processing airkorea data: {e}")
        sys.stdout.flush()

    end_time = time.time() # í•¨ìˆ˜ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    print(f"--- Finished fetch_weather_data. Total time: {end_time - start_time:.2f} seconds. Final weather dict: {weather} ---")
    sys.stdout.flush()
    return weather


def create_weather_card(region_name, weather_data, web_url):
    """ë‚ ì”¨ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹´ì¹´ì˜¤í†¡ ListCardë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"--- Starting create_weather_card for region: {region_name} ---")
    sys.stdout.flush()
    print(f"Received weather_data in create_weather_card: {weather_data}")
    sys.stdout.flush()

    # ê¸°ì˜¨ ë°ì´í„°ê°€ ì—†ê±°ë‚˜, ë‚ ì”¨ ì •ë³´ê°€ ì œëŒ€ë¡œ íŒŒì‹±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
    if not weather_data or not weather_data.get("T1H"): 
        print(f"Weather data incomplete or missing for {region_name}. Returning error message.")
        sys.stdout.flush()
        return {
            "simpleText": {"text": f"'{region_name}' ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
        }

    TMP = weather_data.get("T1H", "-")
    REH = weather_data.get("REH", "-")
    PM10 = weather_data.get("PM10", "-")
    PM25 = weather_data.get("PM25", "-") 
    SKY = weather_data.get("SKY", "1") 
    PTY = weather_data.get("PTY", "0") 

    # ë‚ ì”¨ ìƒíƒœ ë¬¸ìì—´ ìƒì„±
    weather_condition = get_sky_condition(SKY, PTY)
    
    # ë¯¸ì„¸ë¨¼ì§€ ë“±ê¸‰ ë° ë©”ì‹œì§€
    pm10_level, pm10_msg = get_fine_dust_level(PM10, is_pm25=False)
    pm25_level, pm25_msg = get_fine_dust_level(PM25, is_pm25=True)
    
    # ìŠµë„ ë“±ê¸‰ ë° ë©”ì‹œì§€
    reh_level, reh_msg = get_humidity_level(REH)

    print(f"Generated weather card content for {region_name}")
    sys.stdout.flush()
    return {
        "listCard": {
            "header": {"title": f"â˜€ï¸ '{region_name}' í˜„ì¬ ë‚ ì”¨"},
            "items": [
                # ê¸°ì˜¨ í•­ëª©: ê¸°ì˜¨ê³¼ ë‚ ì”¨ ìƒíƒœ í•¨ê»˜ í‘œì‹œ
                {"title": f"ê¸°ì˜¨ {TMP}â„ƒ, {weather_condition}", "description": ""},
                # ë¯¸ì„¸ë¨¼ì§€ í•­ëª©: PM10ê³¼ PM25 ë“±ê¸‰ ë° ë©”ì‹œì§€ í•¨ê»˜ í‘œì‹œ
                {"title": f"ë¯¸ì„¸ë¨¼ì§€: {pm10_level} / ì´ˆë¯¸ì„¸ë¨¼ì§€: {pm25_level}", "description": f"PM10: {pm10_msg}\nPM2.5: {pm25_msg}"},
                # ìŠµë„ í•­ëª©: ë“±ê¸‰ê³¼ í¼ì„¼íŠ¸ í•¨ê»˜ í‘œì‹œ
                {"title": f"ìŠµë„ {reh_level} ({REH}%)", "description": reh_msg},
            ],
            "buttons": [
                {"label": "ë‹¤ë¥¸ ì§€ì—­ ë³´ê¸°", "action": "message", "messageText": "ì§€ì—­ ë³€ê²½í•˜ê¸°"},
                {
                    "label": "ê¸°ìƒì²­ ì „êµ­ ë‚ ì”¨",
                    "action": "webLink",
                    "webLinkUrl": "https://www.weather.go.kr/w/weather/forecast/short-term.do" # ê³ ì •ëœ URL ì‚¬ìš©
                }
            ]
        }
    }


# --- ë¼ìš°íŠ¸ ì •ì˜ ---

@app.route("/news/ask_keyword", methods=["POST"])
def search_by_user_input():
    """ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    body = request.get_json()
    # 'keyword' íŒŒë¼ë¯¸í„° ìš°ì„  í™•ì¸
    keyword = body.get("action", {}).get("params", {}).get("keyword", "").strip()

    # íŒŒë¼ë¯¸í„°ì— 'keyword'ê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ë°œí™”ë¥¼ ì§ì ‘ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©
    if not keyword:
        keyword = body.get("userRequest", {}).get("utterance", "").strip()

    if not keyword:
        return jsonify({
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {"text": "ê²€ìƒ‰ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                }]
            }
        })
    return search_news_response(keyword, max_count=5)

# ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë¼ìš°íŠ¸
@app.route("/news/politics", methods=["POST"])
def news_politics():
    """ì •ì¹˜ ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return list_card_response("ì •ì¹˜", "https://rss.donga.com/politics.xml", "https://www.donga.com/news/Politics")

@app.route("/news/economy", methods=["POST"])
def news_economy():
    """ê²½ì œ ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return list_card_response("ê²½ì œ", "https://rss.donga.com/economy.xml", "https://www.donga.com/news/Economy")

@app.route("/news/society", methods=["POST"])
def news_society():
    """ì‚¬íšŒ ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return list_card_response("ì‚¬íšŒ", "https://rss.donga.com/national.xml", "https://www.donga.com/news/National")

@app.route("/news/world", methods=["POST"])
def news_world():
    """êµ­ì œ ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return list_card_response("êµ­ì œ", "https://rss.donga.com/international.xml", "https://www.donga.com/news/Inter")

@app.route("/news/science", methods=["POST"])
def news_science():
    """IT/ê³¼í•™ ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ "IT ê³¼í•™"ìœ¼ë¡œ ë ˆì´ë¸” ë³€ê²½
    return list_card_response("IT ê³¼í•™", "https://rss.donga.com/science.xml", "https://www.donga.com/news/It")

@app.route("/news/culture", methods=["POST"])
def news_culture():
    """ë¬¸í™” ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ "ë¬¸í™”"ë¡œ ë ˆì´ë¸” ë³€ê²½ ë° RSS/ì›¹ë§í¬ë„ ë¬¸í™”ë¡œ ë³€ê²½ í•„ìš”
    # ë™ì•„ì¼ë³´ RSSì— 'ë¬¸í™”' ë‹¨ë… í”¼ë“œëŠ” ë³´ì´ì§€ ì•Šìœ¼ë¯€ë¡œ, 'ë¬¸í™”ì—°ì˜ˆ' í”¼ë“œë¥¼ ì‚¬ìš©í•˜ê³  ë ˆì´ë¸”ë§Œ 'ë¬¸í™”'ë¡œ í‘œì‹œ
    return list_card_response("ë¬¸í™”", "https://rss.donga.com/culture.xml", "https://www.donga.com/news/Culture")

@app.route("/news/sports", methods=["POST"])
def news_sports():
    """ìŠ¤í¬ì¸  ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return list_card_response("ìŠ¤í¬ì¸ ", "https://rss.donga.com/sports.xml", "https://www.donga.com/news/Sports")

@app.route("/news/entertainment", methods=["POST"])
def news_entertainment():
    """ì—°ì˜ˆ ë‰´ìŠ¤ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return list_card_response("ì—°ì˜ˆ", "https://rss.donga.com/entertainment.xml", "https://www.donga.com/news/Entertainment")

# íŠ¸ë Œë”© ë‰´ìŠ¤ ë¼ìš°íŠ¸
@app.route("/news/trending", methods=["POST"])
def trending_daily():
    """'ìš”ì¦˜ ëœ¨ëŠ” ë‰´ìŠ¤' ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return trending_card_response("ìš”ì¦˜ ëœ¨ëŠ” ë‰´ìŠ¤", "https://www.donga.com/news/TrendNews/daily")

@app.route("/news/popular", methods=["POST"])
def trending_monthly():
    """'ë§ì´ ë³¸ ë‰´ìŠ¤' ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    return trending_card_response("ë§ì´ ë³¸ ë‰´ìŠ¤", "https://www.donga.com/news/TrendNews/monthly")

# ë‚ ì”¨ ì •ë³´ ë¼ìš°íŠ¸ (ê¸°ì¡´ /weather/change-region ìœ ì§€)
@app.route("/weather/change-region", methods=["POST"])
def weather_by_region():
    """ì‚¬ìš©ìê°€ ì„ íƒí•œ ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
    body = request.get_json()
    print(f"Received webhook body for /weather/change-region: {json.dumps(body, indent=2)}") # ì›¹í›… ë°”ë”” ë¡œê¹… ì¶”ê°€
    sys.stdout.flush()

    # 'detailParams'ì—ì„œ 'region_name'ì„ ë¨¼ì € ì‹œë„í•˜ê³ , ì—†ìœ¼ë©´ 'params'ì—ì„œ ì‹œë„
    region = body.get("action", {}).get("detailParams", {}).get("region_name", {}).get("origin", "").strip()
    if not region: # detailParams.originì´ ë¹„ì–´ìˆì„ ê²½ìš° params.region_name í™•ì¸
        region = body.get("action", {}).get("params", {}).get("region_name", "ì„œìš¸").strip()

    print(f"Extracted region for /weather/change-region: {region}") # ì¶”ì¶œëœ ì§€ì—­ëª… ë¡œê¹… ì¶”ê°€
    sys.stdout.flush()

    nx, ny = get_coords(region)

    if not nx or not ny:
        return jsonify({
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {"text": f"'{region}' ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”."}
                }]
            }
        })
    
    # ë¯¸ì„¸ë¨¼ì§€ ë°ì´í„°ë¥¼ ìœ„í•´ ì‹œë„ ì´ë¦„ì„ fetch_weather_dataì— ì „ë‹¬
    weather_data = fetch_weather_data(nx, ny, region_full_name=region) 
    weather_card = create_weather_card(region, weather_data, "https://www.weather.go.kr/w/weather/forecast/short-term.do")

    return jsonify({
        "version": "2.0",
        "template": {
            "outputs": [weather_card]
        }
    })

# /news/weather ë¼ìš°íŠ¸ ì¶”ê°€ (ê¸°ì¡´ /news/briefing ëŒ€ì²´)
@app.route("/news/weather", methods=["POST"])
def news_weather_route():
    """ë‚ ì”¨ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤ (ê¸°ë³¸ ì§€ì—­ ì„œìš¸ ë˜ëŠ” ì‚¬ìš©ì ì§€ì • ì§€ì—­)."""
    body = request.get_json()
    print(f"Received webhook body for /news/weather: {json.dumps(body, indent=2)}") # ì›¹í›… ë°”ë”” ë¡œê¹… ì¶”ê°€
    sys.stdout.flush()

    # 'detailParams'ì—ì„œ 'region_name'ì„ ë¨¼ì € ì‹œë„í•˜ê³ , ì—†ìœ¼ë©´ 'params'ì—ì„œ ì‹œë„
    region = body.get("action", {}).get("detailParams", {}).get("region_name", {}).get("origin", "").strip()
    if not region: # detailParams.originì´ ë¹„ì–´ìˆì„ ê²½ìš° params.region_name í™•ì¸
        region = body.get("action", {}).get("params", {}).get("region_name", "ì„œìš¸").strip()

    print(f"Extracted region for /news/weather: {region}") # ì¶”ì¶œëœ ì§€ì—­ëª… ë¡œê¹… ì¶”ê°€
    sys.stdout.flush()
    
    nx, ny = get_coords(region)

    if not nx or not ny:
        return jsonify({
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {"text": f"'{region}' ì§€ì—­ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
                }]
            }
        })
    
    # ë¯¸ì„¸ë¨¼ì§€ ë°ì´í„°ë¥¼ ìœ„í•´ ì‹œë„ ì´ë¦„ì„ fetch_weather_dataì— ì „ë‹¬
    weather_data = fetch_weather_data(nx, ny, region_full_name=region)
    # create_weather_card í•¨ìˆ˜ê°€ ì´ë¯¸ì§€ì²˜ëŸ¼ ListCardë¥¼ ìƒì„±í•˜ê³  ë²„íŠ¼ í¬í•¨
    weather_card = create_weather_card(region, weather_data, "https://www.weather.go.kr/w/weather/forecast/short-term.do")

    return jsonify({
        "version": "2.0",
        "template": {
            "outputs": [weather_card]
        }
    })

# ìƒˆë¡œìš´ ì•Œë¦¼ ì´ˆê¸°í™” ë©”ì‹œì§€ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
@app.route("/news/handle_alarm_init", methods=["POST"])
def handle_alarm_init_message():
    """
    ì¹´ì¹´ì˜¤í†¡ ì±—ë´‡ ë¹Œë”ì˜ 'ì•Œë¦¼ë°›ê¸°' ë¸”ë¡ì—ì„œ í˜¸ì¶œë˜ëŠ” ì›¹í›…ì…ë‹ˆë‹¤.
    ì»¨í…ìŠ¤íŠ¸ë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ topic íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ì¸ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    body = request.get_json()
    print(f"Received webhook body for /news/handle_alarm_init: {json.dumps(body, indent=2)}")
    sys.stdout.flush()

    topic = ""
    # 1. userRequest.contextsì—ì„œ "news_alarm_context"ë¥¼ ì°¾ì•„ topic ì¶”ì¶œ (ìµœìš°ì„ )
    if body.get("userRequest", {}).get("contexts"):
        for context in body["userRequest"]["contexts"]:
            # context.get("name")ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì´ë¦„ì„ ì•ˆì „í•˜ê²Œ í™•ì¸
            if context.get("name") == "news_alarm_context" and "topic" in context.get("params", {}):
                topic = context["params"]["topic"].strip()
                print(f"Parsed topic from context ('news_alarm_context'): {topic}")
                sys.stdout.flush()
                break # ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ê²€ìƒ‰í•˜ì§€ ì•ŠìŒ
    
    # 2. action.paramsì—ì„œ ì‹œë„ (ë¸”ë¡ íŒŒë¼ë¯¸í„°ê°€ ì›¹í›…ìœ¼ë¡œ ë§¤í•‘ëœ ê²½ìš° - ì»¨í…ìŠ¤íŠ¸ ìœ ì…ì´ ì˜ ì•ˆë  ê²½ìš°ì˜ ëŒ€ë¹„ì±…)
    if not topic:
        topic = body.get("action", {}).get("params", {}).get("topic", "").strip()
        if topic:
            print(f"Parsed topic from action.params: {topic}")
            sys.stdout.flush()

    # 3. userRequest.utteranceì—ì„œ "ë‰´ìŠ¤ì•Œë¦¼ì„¤ì •:TOPIC" íŒ¨í„´ì„ íŒŒì‹± (fallback - í˜¹ì‹œ ëª¨ë¥¼ ëŒ€ë¹„)
    if not topic:
        utterance = body.get("userRequest", {}).get("utterance", "").strip()
        if utterance.startswith("ë‰´ìŠ¤ì•Œë¦¼ì„¤ì •:"):
            topic = utterance.split(":", 1)[1].strip()
            print(f"Parsed topic from utterance (fallback): {topic}")
            sys.stdout.flush()
        else:
            print(f"Utterance does not match 'ë‰´ìŠ¤ì•Œë¦¼ì„¤ì •:' pattern (fallback): {utterance}")
            sys.stdout.flush()

    # 4. userRequest.action.extraì—ì„œ ì‹œë„ (ê³¼ê±° extra ì „ë‹¬ ë°©ì‹, fallback - í˜¹ì‹œ ëª¨ë¥¼ ëŒ€ë¹„)
    if not topic:
        topic = body.get("userRequest", {}).get("action", {}).get("extra", {}).get("topic", "").strip()
        if topic:
            print(f"Parsed topic from userRequest.action.extra (fallback): {topic}")
            sys.stdout.flush()


    if not topic:
        print("Warning: 'topic' parameter not found in webhook request for /news/handle_alarm_init after all attempts.")
        sys.stdout.flush()
        response_text = "ì•Œë¦¼ ì£¼ì œë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    else:
        response_text = f"ì–¸ì œ `{topic}` ë‰´ìŠ¤ë¥¼ ë³´ë‚´ë“œë¦´ê¹Œìš”?\nì›í•˜ëŠ” ë°©ë²•ì„ ì„ íƒí•´ ì£¼ì„¸ìš”."
        print(f"Generated alarm init message for topic: {topic}")
        sys.stdout.flush()

    return jsonify({
        "version": "2.0",
        "template": {
            "outputs": [{
                "simpleText": {"text": response_text}
            }]
        }
    })

# í—¬ìŠ¤ ì²´í¬ ë¼ìš°íŠ¸
@app.route("/", methods=["GET"])
def health():
    """ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” í—¬ìŠ¤ ì²´í¬ ë¼ìš°íŠ¸ì…ë‹ˆë‹¤."""
    return "ì¹´ì¹´ì˜¤ ë‰´ìŠ¤ë´‡ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
